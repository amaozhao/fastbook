{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[chapter_ethics]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据伦理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 侧边栏：致谢：Rachel Thomas 博士"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 本章由fast.ai的联合创始人Rachel Thomas博士和旧金山大学应用数据伦理中心的创始主任共同撰写。它主要遵循她为[数据伦理导论](https://ethics.fast.ai)课程开发的课程大纲的一个子集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结束侧边栏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 正如我们在第1章和第2章中讨论的，机器学习模型有时会出错。它们可能有缺陷。它们可能会遇到之前未见过的数据，并以我们意想不到的方式行为。或者它们可能完全按照设计工作，但被用于我们宁愿它们从未被用于的事情。\n",
    "\n",
    "因为深度学习是一个非常强大的工具，可以用于许多事情，所以我们特别需要考虑我们选择的后果。哲学的伦理学研究是对与错的研究，包括我们如何定义这些术语，识别对与错的行为，以及理解行为与后果之间的联系。数据伦理领域已经存在很长时间，有许多学者专注于这个领域。它被用来帮助许多司法管辖区定义政策；它被大小公司用来考虑如何最好地确保产品开发带来良好的社会结果；它被研究人员用来确保他们的工作被用于好的用途，而不是坏的用途。\n",
    "\n",
    "因此，作为一个深度学习从业者，你可能在某个时候会面临需要考虑数据伦理的情况。那么什么是数据伦理呢？它是伦理学的一个子领域，让我们从这里开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > J: 在大学时，伦理哲学是我主要的研究领域（如果我没有辍学加入现实世界，它本会是我的论文主题）。根据我多年学习伦理的经验，我可以告诉你：没有人真正同意什么是对和错，它们是否存在，如何识别它们，哪些人是好人，哪些是坏人，或者几乎任何其他事情。所以不要对理论期望太高！我们将专注于这里的例子和思考启发，而不是理论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在回答[\"What Is Ethics\"](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/what-is-ethics/)这个问题时，Markkula应用伦理中心表示，这个术语指的是：\n",
    "\n",
    "- 有根据的对与错的标准，规定人类应该做什么\n",
    "- 研究和发展个人的伦理标准。\n",
    "\n",
    "没有正确答案的清单。没有做与不做的清单。伦理是复杂的，依赖于上下文。它涉及许多利益相关者的观点。伦理是你必须要发展和练习的一种能力。在本章中，我们的目标是提供一些路标，帮助你在这段旅程中前行。\n",
    "\n",
    "发现伦理问题最好是作为协作团队的一部分来进行。这是你真正能够融入不同观点的唯一方式。不同人的背景将帮助他们看到对你来说可能不明显的事物。与团队合作对于许多“能力建设”活动都是有帮助的，包括这一项。\n",
    "\n",
    "本章当然不是本书中我们讨论数据伦理的唯一部分，但有一个专门关注它的部分是很好的。为了定位，也许最容易的方法是看一些例子。因此，我们挑选了三个我们认为能有效说明一些关键主题的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据伦理的关键例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 我们将从三个具体的例子开始，这些例子说明了技术领域中常见的三个伦理问题：\n",
    "\n",
    "1. **救济流程** - 阿肯色州有缺陷的医疗保健算法使患者陷入困境。\n",
    "2. **反馈循环** - YouTube的推荐系统帮助引发了阴谋论的热潮。\n",
    "3. **偏见** - 当在谷歌上搜索一个传统的非洲裔美国人的名字时，它会显示犯罪背景检查的广告。\n",
    "\n",
    "实际上，对于本章中我们介绍的每一个概念，我们都会提供一个具体的例子。对于每一个例子，想想你在这种情况下可以做什么，以及可能有哪些障碍阻止你完成这些工作。你会如何处理它们？你会注意什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 错误和救济：用于医疗福利的有缺陷算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Verge调查了在美国超过一半的州使用的一款软件，该软件用于决定人们获得多少医疗保健，并将他们的发现记录在文章\"What Happens When an Algorithm Cuts Your Healthcare\"中。在阿肯色州实施该算法后，数百人（许多有严重残疾）的医疗保健被大幅削减。例如，患有脑瘫的Tammy Dobbs女士需要助手帮助她起床、上厕所、获取食物等，她的帮助时间突然减少了每周20小时。她无法得到任何解释为什么她的医疗保健被削减。最终，一起法庭案件揭示了算法软件实施中存在错误，对糖尿病患者或脑瘫患者产生了负面影响。然而，Dobbs和许多依赖这些医疗保健福利的人生活在恐惧中，担心他们的福利可能会再次突然且无法解释地被削减。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反馈循环：YouTube的推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你的模型控制着你获得的下一轮数据时，就可能发生反馈循环。返回的数据很快就会被软件本身所影响。\n",
    "\n",
    "例如，YouTube拥有19亿用户，他们每天观看超过10亿小时的YouTube视频。其推荐算法（由Google构建），旨在优化观看时间，负责大约70%的观看内容。但存在一个问题：它导致了失控的反馈循环，导致《纽约时报》以标题[\"YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?\"](https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html)进行了报道。表面上，推荐系统预测人们会喜欢什么内容，但它们在决定人们甚至看到什么内容方面也有很大的权力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏见：拉塔尼亚·斯威尼教授“被捕”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dr. Latanya Sweeney是哈佛大学的教授，也是该大学数据隐私实验室的主任。在论文[\"Discrimination in Online Ad Delivery\"](https://arxiv.org/abs/1301.6822)（见<<latanya_arrested>>)中，她描述了自己的发现：搜索她的名字时，出现了广告说“Latanya Sweeney，被捕了？”尽管她是唯一已知的Latanya Sweeney，并且从未被捕过。然而，当她搜索其他名字，如“Kirsten Lindquist”时，她得到了更中性的广告，尽管Kirsten Lindquist已经被捕三次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image1.png\" id=\"latanya_arrested\" caption=\"Google search showing ads about Professor Latanya Sweeney's arrest record\" alt=\"Screenshot of google search showing ads about Professor Latanya Sweeney's arrest record\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 作为一名计算机科学家，她系统地研究了这个问题，并查看了超过2000个名字。她发现了一个明显的模式，历史上与黑人相关的名字会收到暗示该人有犯罪记录的广告，而白人名字则收到更中性的广告。\n",
    "\n",
    "这是一个偏见的例子。这可能对人们的生活产生重大影响——例如，如果一个求职者被谷歌搜索，可能会显示他们有犯罪记录，而实际上他们并没有。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么这很重要？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对这些问题的一个很自然的反应是：“那又怎样？这跟我有什么关系？我是一个数据科学家，不是政治家。我不是公司里做决策的高级执行官。我只是尽力构建最有预测力的模型。”\n",
    "\n",
    "这些都是非常合理的问题。但我们要试图说服你，答案是每个训练模型的人都绝对需要考虑他们的模型将如何被使用，并考虑如何最好地确保它们的使用尽可能积极。你可以做一些事。如果你不做，那么事情可能会变得很糟糕。\n",
    "\n",
    "一个特别可怕的例子，说明了当技术专家不顾一切地关注技术时会发生什么，是IBM和纳粹德国的故事。2001年，一位瑞士法官裁定，“推断IBM的技术援助促进了纳粹在犯下反人类罪行时的任务，这是合理的，这些行为也涉及了IBM机器的会计和分类，并在集中营中被利用。”\n",
    "\n",
    "你知道，IBM向纳粹提供了必要的数据制表产品，以大规模跟踪犹太人和其他群体的灭绝。这是公司最高层推动的，包括向希特勒和他的领导团队进行市场营销。公司总裁托马斯·沃森亲自批准了1939年发布特殊的IBM字母排序机，以帮助组织波兰犹太人的驱逐。在<<meeting>>中可以看到阿道夫·希特勒（最左边）与IBM首席执行官汤姆·沃森（从左数第二）会面，就在希特勒在1937年授予沃森“为帝国服务”勋章前不久。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image2.png\" id=\"meeting\" caption=\"IBM CEO Tom Watson Sr. meeting with Adolf Hitler\" alt=\"A picture of IBM CEO Tom Watson Sr. meeting with Adolf Hitler\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 但这并不是孤立的事件——该组织的参与非常广泛。IBM及其子公司在集中营现场定期提供培训和维护：打印卡片，配置机器，并在它们频繁出现故障时进行修理。IBM在其穿孔卡片系统中为每个人的死亡方式、他们被分配的群体以及通过庞大的大屠杀系统追踪他们所需的物流信息设置了分类。IBM在集中营中犹太人的代码是8：大约有600万人被杀害。其对罗姆人的代码是12（他们被纳粹标记为“反社会者”，在“吉普赛营”或“Zigeunerlager”中有超过30万人被杀害）。普通执行的代码是4，毒气室死亡的代码是6。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image3.jpeg\" id=\"punch_card\" caption=\"A punch card used by IBM in concentration camps\" alt=\"Picture of a punch card used by IBM in concentration camps\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 当然，参与其中的项目经理、工程师和技术员只是在过他们的日常生活。照顾他们的家庭，周日去教堂，尽他们所能做好工作。听从命令。市场营销人员只是尽力实现他们的业务发展目标。正如《IBM与大屠杀》（Dialog Press）的作者埃德温·布莱克（Edwin Black）所观察到的：“对于盲目的技术官僚来说，手段比目的更重要。犹太人的毁灭变得甚至不那么重要了，因为IBM技术成就的振奋性质在那个时候通过惊人的利润得到了加强，当时全世界都在排长队领面包。”\n",
    "\n",
    "退后一步，考虑一下：如果你发现自己成为了一个最终伤害社会的系统的一部分，你会有什么感受？你会愿意发现吗？你如何帮助确保这种情况不会发生？我们在这里描述了最极端的情况，但今天与人工智能和机器学习相关的许多负面社会后果正在被观察到，我们将在本章中描述其中的一些。\n",
    "\n",
    "这不仅仅是道德负担。有时，技术专家会直接为他们的行为付出代价。例如，在大众汽车丑闻中，第一个被监禁的人不是监督项目的经理，也不是公司的高管。而是工程师詹姆斯·梁（James Liang），他只是按照吩咐行事。\n",
    "\n",
    "当然，并非都是坏事——如果你参与的项目最终对哪怕一个人产生了巨大的积极影响，这会让你感觉很棒！\n",
    "\n",
    "好吧，我们希望你已经被说服，认为你应该关心。但你应该怎么做呢？作为数据科学家，我们天生倾向于通过优化某个指标来改进我们的模型。但优化那个指标可能实际上并不会导致更好的结果。即使它确实有助于创造更好的结果，它几乎肯定不是唯一重要的事。考虑从研究人员或实践者开发模型或算法到实际用于做出某些决策之间的步骤管道。如果我们希望获得我们想要的结果，那么整个管道都需要作为一个整体来考虑。\n",
    "\n",
    "通常，从一端到另一端有一个非常长的链条。这在你是研究员时尤其如此，你可能甚至不知道你的研究是否会被用于任何事情，或者你参与数据收集，这在管道中更早。但没有人比你更适合告知这个链条中的所有人关于你工作的能力、限制和细节。虽然没有“银弹”可以确保你的工作被正确使用，但通过参与过程并提出正确的问题，你至少可以确保正在考虑正确的问题。\n",
    "\n",
    "有时，对于被要求做一项工作的正确回应就是直接说“不”。然而，我们经常听到的回应是，“如果我不做，别人会做。”但考虑一下这一点：如果你被选中做这份工作，你是他们找到的最合适的人——所以如果你不做，最合适的人就不会在这个项目上工作。如果他们问的前五个人也都说不，那就更好了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将机器学习与产品设计整合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你之所以做这项工作，大概是因为你希望它能被用于某种目的。否则，你就是在浪费时间。所以，让我们从你的工作最终会有所归属的假设开始。现在，当你收集数据和开发模型时，你正在做出许多决策。你会在哪个层次上存储你的数据？你应该使用哪种损失函数？你应该使用哪些验证和训练集？你应该专注于实现的简单性、推理的速度，还是模型的准确性？你的模型将如何处理域外数据项？它可以进行微调，还是必须从头开始重新训练？\n",
    "\n",
    "这些不仅仅是算法问题。它们是数据产品设计问题。但产品经理、高管、法官、记者、医生……最终开发和使用你模型所属系统的任何人，都不太可能理解你所做出的决策，更不用说改变它们了。\n",
    "\n",
    "例如，两项研究发现，亚马逊的面部识别软件产生了不准确和种族偏见的结果。亚马逊声称研究人员应该更改默认参数，但没有解释这将如何改变偏见结果。此外，事实证明，亚马逊也没有指导使用其软件的警察部门这样做。开发这些算法的研究人员与撰写提供给警察的指南的亚马逊文档人员之间，可能存在很大的距离。缺乏紧密整合导致了对整个社会、警察和亚马逊自身的严重问题。结果发现，他们的系统错误地将28名国会议员与罪犯的面部照片匹配！（而且错误匹配到罪犯面部照片的国会议员中，有色人种的比例不成比例地高，如<<congressmen>>中所见。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image4.png\" id=\"congressmen\" caption=\"Congresspeople matched to criminal mugshots by Amazon software\" alt=\"Picture of the congresspeople matched to criminal mugshots by Amazon software, they are disproportionately people of color\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 数据科学家需要成为一个跨学科团队的一部分。研究人员需要与最终使用他们研究的人密切合作。更好的情况是，如果领域专家自己已经学到了足够的知识，能够自己训练和调试一些模型——希望现在正在阅读这本书的你们中有一些是这样的专家！\n",
    "\n",
    "现代工作场所是一个非常专业化的地方。每个人往往有明确定义的工作要执行。特别是在大公司中，了解所有拼图的碎片可能很难。有时，如果公司知道员工不会喜欢答案，甚至会有意地尽可能地隐藏正在努力实现的整体项目目标。这有时是通过尽可能地分隔各个部分来完成的。\n",
    "\n",
    "换句话说，我们并不是说这一切都很容易。这很难。真的很难。我们都必须尽我们所能。而且我们经常看到，那些参与这些项目更高层次背景的人，尝试发展跨学科能力和团队的人，成为他们组织中最重要、最受奖励的成员之一。这种工作往往受到高层管理人员的高度赞赏，即使有时被中层管理层认为相当不舒服。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据伦理的主题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据伦理是一个很大的领域，我们无法涵盖所有内容。相反，我们会选择一些我们认为特别相关的话题：\n",
    "\n",
    "- 救济和问责的必要性\n",
    "- 反馈循环\n",
    "- 偏见\n",
    "- 虚假信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们逐一看看每个话题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 救济和问责"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在复杂系统中，没有人觉得自己对结果负责是很容易的。虽然这是可以理解的，但这并不会产生好的结果。在前面提到的阿肯色州医疗保健系统的例子中，一个错误导致脑瘫患者失去了所需的护理，算法的创建者责怪政府官员，政府官员责怪那些实施软件的人。纽约大学教授Danah Boyd描述了这一现象：“官僚制经常被用来转移或逃避责任...今天的算法系统正在扩展官僚制。”\n",
    "\n",
    "救济如此必要的另一个原因是数据通常包含错误。审计和错误纠正机制至关重要。加利福尼亚州执法官员维护的一个疑似帮派成员数据库被发现充满了错误，包括42名不到1岁的婴儿被添加到数据库中（其中28人被标记为“承认自己是帮派成员”）。在这种情况下，一旦人们被加入，就没有纠正错误或移除人员的流程。另一个例子是美国的信用报告系统：联邦贸易委员会（FTC）在2012年对信用报告进行的大规模研究发现，26%的消费者在他们的档案中至少有一个错误，5%的人有可能导致灾难性后果的错误。然而，纠正这些错误的流程非常缓慢且不透明。当公共广播记者Bobby Allyn发现自己被错误地列为拥有枪支定罪时，他花了“十几通电话、一个县法院书记员的帮助和六周时间来解决问题。而这只是在我作为记者联系公司的公关部门之后。”\n",
    "\n",
    "作为机器学习从业者，我们并不总是认为理解我们的算法最终如何在实践中被实施是我们的责任。但我们需要这样做。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反馈循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 我们在<<chapter_intro>>中解释了算法如何与其环境互动以创建反馈循环，做出预测，这些预测会强化现实世界中采取的行动，从而导致预测在相同方向上更加明显。\n",
    "\n",
    "作为例子，让我们再次考虑YouTube的推荐系统。几年前，谷歌团队谈到了他们如何引入强化学习（与深度学习密切相关，但损失函数代表的结果是行动发生后很长时间才可能产生的）来改进YouTube的推荐系统。他们描述了他们如何使用一种算法进行推荐，以优化观看时间。\n",
    "\n",
    "然而，人类往往被争议性内容所吸引。这意味着关于阴谋论等事物的视频开始被推荐系统越来越多地推荐。此外，事实证明，对阴谋论感兴趣的人也是观看大量在线视频的人！因此，他们开始越来越多地被吸引到YouTube。阴谋论者在YouTube上观看视频的人数增加，导致算法推荐越来越多的阴谋论和其他极端内容，这导致越来越多的极端主义者在YouTube上观看视频，越来越多的人在YouTube上观看视频发展出极端观点，这导致算法推荐更多的极端内容……系统失控了。\n",
    "\n",
    "而这种现象并不局限于特定类型的内容。2019年6月，《纽约时报》发表了一篇关于YouTube推荐系统的文章，标题为[\"On YouTube’s Digital Playground, an Open Gate for Pedophiles\"](https://www.nytimes.com/2019/06/03/world/americas/youtube-pedophiles.html)。文章以这个令人毛骨悚然的故事开头："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > : 当Christiane C.的10岁女儿和她的朋友上传了一段在后院游泳池玩耍的视频时，她并没有多想……几天后……这个视频已经有成千上万的观看次数。不久之后，观看次数上升到了40万……“我再次看到视频，被观看次数吓到了，”Christiane说。她有理由感到害怕。YouTube的自动推荐系统……已经开始向观看其他青春期前、部分着装的儿童视频的用户展示这个视频，研究人员团队发现。\n",
    "\n",
    "> : 单独来看，每个视频可能完全是无辜的，比如孩子制作的家用录像。任何暴露的画面都是短暂的，看起来是偶然的。但是，当它们被组合在一起时，它们的共同特征变得非常明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " YouTube的推荐算法开始为恋童癖策划播放列表，挑选出偶然包含青春期前、部分着装儿童的无辜家庭视频。\n",
    "\n",
    "没有人在谷歌计划创建一个将家庭视频变成恋童癖色情内容的系统。那么发生了什么呢？\n",
    "\n",
    "问题的一部分在于，指标在推动财务重要系统方面的核心地位。当一个算法有一个要优化的指标时，正如你所看到的，它会尽一切努力优化那个数字。这往往会导致各种边缘情况，与系统互动的人类会寻找、发现并利用这些边缘情况和反馈循环来获得优势。\n",
    "\n",
    "有迹象表明，这正是YouTube推荐系统发生的事情。《卫报》发表了一篇名为[\"How an ex-YouTube Insider Investigated its Secret Algorithm\"](https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot)的文章，讲述了前YouTube工程师Guillaume Chaslot如何创建AlgoTransparency来追踪这些问题。Chaslot在罗伯特·穆勒的《2016年总统选举俄罗斯干预调查报告》发布后发布了<<ethics_yt_rt>>中的图表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image18.jpeg\" id=\"ethics_yt_rt\" caption=\"Coverage of the Mueller report\" alt=\"Coverage of the Mueller report\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 今日俄罗斯对穆勒报告的报道在推荐频道数量上是一个极端的异常值。这表明，今日俄罗斯，一个国有的俄罗斯媒体机构，可能成功地操纵了YouTube的推荐算法。不幸的是，像这样的系统的不透明性使得很难揭露我们正在讨论的问题。\n",
    "\n",
    "本书的一位审稿人Aurélien Géron在2013年至2016年（远早于这里讨论的事件）领导了YouTube的视频分类团队。他指出，不仅仅是涉及人类的反馈循环才是问题。也可能存在没有人的反馈循环！他向我们讲述了YouTube的一个例子：\n",
    "\n",
    "> : 确定视频主要主题的一个重要信号是它来自哪个频道。例如，上传到烹饪频道的视频很可能是烹饪视频。但我们怎么知道一个频道的主题是什么？嗯……部分是通过查看它包含的视频的主题！你看到循环了吗？例如，许多视频有描述，指出拍摄视频使用的相机。因此，这些视频中的一些可能被归类为关于“摄影”的视频。如果一个频道有这样的错误分类视频，它可能被归类为“摄影”频道，使得这个频道上未来的视频更有可能被错误地归类为“摄影”。这甚至可能导致失控的病毒式分类！打破这个反馈循环的一种方法是在有和没有频道信号的情况下对视频进行分类。然后在对频道进行分类时，你只能使用没有频道信号获得的类别。这样，反馈循环就被打破了。\n",
    "\n",
    "有积极的例子表明，人们和组织试图解决这些问题。Meetup的首席机器学习工程师Evan Estola[讨论了](https://www.youtube.com/watch?v=MqoRzNhrTnQ)男性比女性对技术聚会更感兴趣的情况。因此，考虑性别可能会导致Meetup的算法向女性推荐更少的技术聚会，结果，更少的女性会发现并参加技术聚会，这可能会导致算法向女性推荐更少的技术聚会，如此循环。因此，Evan和他的团队做出了道德决定，他们的推荐算法不创建这样的反馈循环，明确不在那部分模型中使用性别。看到一家公司不仅仅无意识地优化一个指标，而是考虑其影响，这是令人鼓舞的。据Evan说，“你需要决定不在算法中使用哪个特征……最优化的算法可能不是最适合投入生产的。”\n",
    "\n",
    "虽然Meetup选择避免这样的结果，但Facebook提供了一个允许失控的反馈循环肆意发展的例子。像YouTube一样，它倾向于通过向对一个阴谋论感兴趣的用户介绍更多来激进化用户。正如虚假信息传播研究员Renee DiResta[写道](https://www.fastcompany.com/3059742/social-network-algorithms-are-distorting-reality-by-boosting-conspiracy-theories)："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > : 一旦人们加入一个单一的阴谋论[Facebook]群组，他们就会被算法引导到许多其他群组。加入一个反疫苗群组，你的建议将包括反转基因、化学尾迹观察、地平说（是的，真的），以及“自然治愈癌症群组。推荐引擎不是将用户从兔子洞中拉出来，而是将他们推得更远。”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 极其重要的是要记住，这种行为可能会发生，并且在你的项目中看到第一个迹象时，要么预期一个反馈循环，要么采取积极行动来打破它。另一个需要记住的事情是*偏见*，正如我们在前一章简要讨论的，偏见可以以非常麻烦的方式与反馈循环相互作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在线讨论偏见往往会很快变得非常混乱。\"偏见\"这个词有如此多的不同含义。统计学家通常认为，当数据伦理学家谈论偏见时，他们正在谈论统计学中偏见的定义。但他们不是。他们当然也不是在谈论出现在模型权重和偏差中的偏见，这些是模型的参数！\n",
    "\n",
    "他们谈论的是社会科学中的偏见概念。在[\"A Framework for Understanding Unintended Consequences of Machine Learning\"](https://arxiv.org/abs/1901.10002)中，麻省理工学院的Harini Suresh和John Guttag描述了机器学习中的六种偏见类型，在他们的论文<<bias>>中总结了这些内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/pipeline_diagram.svg\" id=\"bias\" caption=\"Bias in machine learning can come from multiple sources (courtesy of Harini Suresh and John V. Guttag)\" alt=\"A diagram showing all sources where bias can appear in machine learning\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 我们将讨论这四种类型的偏见，我们发现它们在我们的工作中最有帮助（有关其他类型的详细信息，请参见论文）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 历史偏见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 历史偏见源于人们的偏见、过程的偏见以及社会的偏见。Suresh和Guttag指出：“历史偏见是数据生成过程第一步的根本性、结构性问题，即使在完美抽样和特征选择的情况下，它也可能存在。”\n",
    "\n",
    "例如，以下是《纽约时报》文章《即使我们有良好的意图，种族偏见依然存在》中提到的美国历史上的几个种族偏见的例子，该文章由芝加哥大学的Sendhil Mullainathan撰写：\n",
    "\n",
    "- 当医生们看到相同的病历时，他们向黑人患者推荐心脏导管插入术（一种有益的手术）的可能性要小得多。\n",
    "- 在二手车讨价还价时，黑人得到的初始报价比白人高出700美元，并且得到的让步要小得多。\n",
    "- 在Craigslist上回应公寓租赁广告时，使用黑人名字的回应比使用白人名字的回应少。\n",
    "- 全白人陪审团定罪黑人被告的可能性比定罪白人被告高出16个百分点，但当陪审团中有一名黑人成员时，两者的定罪率相同。\n",
    "\n",
    "在美国广泛用于判决和保释决定的COMPAS算法是一个重要算法的例子。当[ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)对其进行测试时，显示在实践中存在明显的种族偏见。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image6.png\" id=\"bail_algorithm\" caption=\"Results of the COMPAS algorithm\" alt=\"Table showing the COMPAS algorithm is more likely to give bail to white people, even if they re-offend more\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 任何涉及人类的数据集都可能存在这种偏见：医疗数据、销售数据、住房数据、政治数据等等。由于潜在偏见无处不在，数据集中的偏见也非常普遍。甚至在计算机视觉领域，种族偏见也会出现，正如Twitter上一位Google Photos用户分享的自动分类照片的例子所展示的那样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image7.png\" id=\"google_photos\" caption=\"One of these labels is very wrong...\" alt=\"Screenshot of the use of Google photos labeling a black user and her friend as gorillas\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 是的，这展示了你认为它是什么：Google Photos将一位黑人用户的与朋友的照片归类为“大猩猩”！这个算法失误在媒体上引起了很大关注。“我们对发生这样的事情感到震惊和真诚地道歉，”公司发言人说。“在自动图像标签方面显然还有很多工作要做，我们正在研究如何防止这类错误在未来发生。”\n",
    "\n",
    "不幸的是，当输入数据存在问题时，修复机器学习系统中的问题是困难的。正如《卫报》的报道所暗示的那样，谷歌的首次尝试并没有激发信心（<<gorilla-ban>>）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image8.png\" id=\"gorilla-ban\" caption=\"Google's first response to the problem\" alt=\"Pictures of a headlines from the Guardian, showing Google removed gorillas and other moneys from the possible labels of its algorithm\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 这类问题当然不仅仅局限于谷歌。麻省理工学院的研究人员研究了最受欢迎的在线计算机视觉API，以了解它们的准确性如何。但他们并没有只计算一个单一的准确率数字——相反，他们查看了四个不同群体的准确性，如<<face_recognition>>所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image9.jpeg\" id=\"face_recognition\" caption=\"Error rate per gender and race for various facial recognition systems\" alt=\"Table showing how various facial recognition systems perform way worse on darker shades of skin and females\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 例如，IBM的系统在较深肤色女性上的误识率达到了34.7%，而较浅肤色男性的误识率仅为0.3%——错误率高出100多倍！然而，一些人错误地对这些实验结果做出反应，声称这种差异仅仅是因为较深的肤色对计算机来说更难识别。然而，实际情况是，在这一结果造成的负面舆论之后，所有相关公司都显著改进了他们对较深肤色的模型，以至于一年后，这些模型对较深肤色的表现几乎与较浅肤色一样好。所以，这实际上表明开发者未能使用包含足够多较深肤色面孔的数据集，或者在测试产品时没有使用较深肤色的面孔。\n",
    "\n",
    "麻省理工学院的研究人员Joy Buolamwini警告说：“我们已经进入了自动化时代，但过于自信却准备不足。如果我们不能制造出道德和包容的人工智能，我们就有可能在机器中立的幌子下失去在民权和性别平等方面取得的成就。”\n",
    "\n",
    "问题的一部分似乎在于，用于训练模型的流行数据集的构成存在系统性的不平衡。Shreya Shankar等人的论文[\"No Classification Without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\"](https://arxiv.org/abs/1711.08536)的摘要指出，“我们分析了两个大型、公开可用的图像数据集，以评估地理多样性，并发现这些数据集似乎表现出可观察的美洲中心和欧洲中心的代表性偏见。此外，我们分析了在这些数据集上训练的分类器，以评估这些训练分布的影响，并发现在不同地区图像的相对表现上有显著差异。”<<image_provenance>>展示了论文中的一个图表，显示了当时（并且仍然如此，因为这本书正在撰写中）用于训练模型的两个最重要的图像数据集的地理构成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image10.png\" id=\"image_provenance\" caption=\"Image provenance in popular training sets\" alt=\"Graphs showing how the vast majority of images in popular training datasets come from the US or Western Europe\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 大部分图像来自美国和其他西方国家，导致在ImageNet上训练的模型在识别其他国家和文化的场景时表现不佳。例如，研究发现这些模型在识别来自低收入国家的家庭用品（如肥皂、香料、沙发或床）方面表现较差。<< object_detect >>展示了Facebook AI Research的Terrance DeVries等人在论文[\"Does Object Recognition Work for Everyone?\"](https://arxiv.org/pdf/1906.02659.pdf)中使用的一张图片，说明了这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image17.png\" id=\"object_detect\" caption=\"Object detection in action\" alt=\"Figure showing an object detection algorithm performing better on western products\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在这个例子中，我们可以看到，低收入国家的肥皂示例在准确性上相差甚远，每个商业图像识别服务都将“食物”预测为最可能的答案！\n",
    "\n",
    "正如我们即将讨论的，此外，绝大多数人工智能研究人员和开发者是年轻的白人男性。我们所看到的大多数项目在进行用户测试时，主要使用产品开发团队的亲朋好友。鉴于此，我们刚刚讨论的这类问题不应该令人感到惊讶。\n",
    "\n",
    "类似的历史偏见也存在于用作自然语言处理模型数据的文本中。这在下游机器学习任务中以多种方式出现。例如，[有广泛报道](https://nypost.com/2017/11/30/google-translates-algorithm-has-a-gender-bias/)称，直到去年，谷歌翻译在将土耳其性别中立代词“o”翻译成英语时表现出系统性偏见：当用于通常与男性相关的职业时，它使用“he”，当用于通常与女性相关的职业时，它使用“she”（<<turkish_gender>>）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image11.png\" id=\"turkish_gender\" caption=\"Gender bias in text data sets\" alt=\"Figure showing gender bias in data sets used to train language models showing up in translations\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 我们也在在线广告中看到了这种偏见。例如，2019年穆罕默德·阿里等人的一项[研究](https://arxiv.org/abs/1904.02095) 发现，即使投放广告的人没有故意歧视，Facebook也会根据种族和性别向非常不同的受众展示广告。具有相同文本的住房广告，但如果图片展示的是白人或黑人家庭，会被展示给种族不同的观众。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测量偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在《美国经济评论》的论文[\"Does Machine Learning Automate Moral Hazard and Error\"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf)中，Sendhil Mullainathan和Ziad Obermeyer研究了一个模型，试图回答这样一个问题：使用历史电子健康记录（EHR）数据，哪些因素最能预测中风？这些是模型中的顶级预测因素：\n",
    "\n",
    "- 既往中风\n",
    "- 心血管疾病\n",
    "- 意外伤害\n",
    "- 良性乳腺肿块\n",
    "- 结肠镜检查\n",
    "- 鼻窦炎\n",
    "\n",
    "然而，只有前两个与中风有关！根据我们目前所学，你或许能猜到为什么。我们实际上并没有真正测量“中风”，中风是由于脑部某个区域因血液供应中断而被剥夺氧气。我们测量的是谁有症状，去看医生，进行了适当的检查，并被诊断出中风。实际上中风并不是唯一与这个完整列表相关的事情——它还与是那种真正会去看医生的人有关（这受到谁能够获得医疗保健、能否负担得起共同支付费用、是否经历基于种族或性别的医疗歧视等因素的影响）！如果你因为*意外伤害*而有可能去看医生，那么当你中风时，你也很可能去看医生。\n",
    "\n",
    "这是*测量偏差*的一个例子。当我们的模型因为我们测量错误的事物、以错误的方式测量，或者不恰当地将这种测量纳入模型而出错时，就会发生测量偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 聚合偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚合偏差（Aggregation bias）发生在模型没有以一种包含所有适当因素的方式聚合数据，或者当模型没有包含必要的交互项、非线性关系等时。这种情况特别可能在医疗环境中发生。例如，糖尿病的治疗方法通常基于简单的单变量统计和涉及小规模异质人群的研究。结果分析通常是在不考虑不同种族或性别的情况下进行的。然而，事实证明，不同种族的糖尿病患者有不同的并发症，并且用于诊断和监测糖尿病的糖化血红蛋白（HbA1c）水平在不同种族和性别中以复杂的方式存在差异。这可能导致人们被误诊或错误治疗，因为医疗决策是基于一个不包含这些重要变量和交互作用的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 表征偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 论文[\"Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting\"](https://arxiv.org/abs/1901.09451)的摘要中，Maria De-Arteaga等人指出，职业中存在性别不平衡（例如，女性更可能成为护士，男性更可能成为牧师），并表示：“不同性别之间的真正阳性率差异与职业中现有的性别不平衡相关，这可能会加剧这些不平衡。”\n",
    "\n",
    "换句话说，研究人员注意到，预测职业的模型不仅反映了潜在人口中的实际性别不平衡，而且实际上还*放大*了这种不平衡！这种类型的*表示偏差*相当常见，尤其是对于简单模型。当存在某种清晰、容易看到的潜在关系时，简单模型通常会简单地假设这种关系始终成立。正如论文中的<<representation_bias>>所示，对于女性占比较高职业，模型倾向于高估该职业的普遍性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image12.png\" id=\"representation_bias\" caption=\"Model error in predicting occupation plotted against percentage of women in said occupation\" alt=\"Graph showing how model predictions overamplify existing bias\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，在训练数据集中，14.6%的外科医生是女性，然而在模型预测中，只有11.6%的真正阳性结果是女性。因此，模型放大了训练集中存在的偏见。\n",
    "\n",
    "现在我们已经看到了这些偏见的存在，我们可以采取什么措施来减轻它们呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决不同类型的偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 不同类型的偏见需要不同的缓解方法。虽然收集更多样化的数据集可以解决代表性偏见，但这并不能帮助解决历史偏见或测量偏差。所有数据集都包含偏见。不存在完全去偏见的数据集。该领域的许多研究人员一直在提出一系列建议，以更好地记录决策、背景和关于特定数据集是如何创建的、适用于哪些场景以及其局限性的具体信息。这样，使用特定数据集的人就不会因其偏见和局限性而措手不及。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 我们经常听到这样的问题——“人类是有偏见的，那么算法偏见真的重要吗？”这个问题被提出得如此频繁，一定有一些理由让提出这个问题的人觉得它是有道理的，但对我们来说，它似乎在逻辑上并不成立！不管这个问题在逻辑上是否站得住脚，重要的是要意识到算法（尤其是机器学习算法！）和人是不同的。考虑以下几点关于机器学习算法的观点：\n",
    "\n",
    "- **机器学习可以创建反馈循环**：少量的偏见可以通过反馈循环迅速指数级增长。\n",
    "- **机器学习可以放大偏见**：人类的偏见可能导致更多的机器学习偏见。\n",
    "- **算法和人类在实际使用中不同**：人类决策者和算法决策者在实践中并不是以即插即用、可互换的方式使用。\n",
    "- **技术是力量**：随之而来的是责任。\n",
    "\n",
    "正如阿肯色州医疗保健的例子所示，机器学习通常在实践中被实施，并不是因为它能带来更好的结果，而是因为它更便宜、更高效。凯西·奥尼尔（Cathy O'Neill）在她的书《数学武器》（Crown）中描述了一种模式，即特权阶层由人处理，而穷人则由算法处理。这只是算法与人类决策者不同使用方式的众多方式之一。其他方式包括：\n",
    "\n",
    "- 人们更可能认为算法是客观的或无错误的（即使他们有选择人类覆盖的选项）。\n",
    "- 算法更可能在没有上诉程序的情况下实施。\n",
    "- 算法通常被大规模使用。\n",
    "- 算法系统成本低廉。\n",
    "\n",
    "即使在没有偏见的情况下，算法（尤其是深度学习，因为它是一种如此有效且可扩展的算法）也可能导致负面的社会问题，例如当它被用于*虚假信息*传播时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 虚假信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 虚假信息（Disinformation）的历史可以追溯数百年甚至数千年。它并不一定是为了让某人相信某些虚假的事情，而是通常用来制造不和谐和不确定性，并让人们放弃寻求真相。接收到相互矛盾的描述可能导致人们认为他们永远无法知道该信任谁或什么。\n",
    "\n",
    "有些人认为虚假信息主要是关于错误的信息或“假新闻”，但实际上，虚假信息往往包含真实的种子，或者是被断章取义的半真半假的信息。拉迪斯拉夫·比特曼（Ladislav Bittman）曾是苏联的情报官员，后来叛逃到美国，并在20世纪70年代和80年代写了一些关于虚假信息在苏联宣传行动中作用的书籍。在《克格勃与苏联虚假信息》（Pergamon）一书中，他写道：“大多数行动都是事实、半真半假、夸大和故意谎言的精心设计混合。”\n",
    "\n",
    "近年来，这在美国已经变得非常贴近现实，联邦调查局详细描述了与2016年选举有关的、与俄罗斯有关的大规模虚假信息活动。了解这次活动中使用的虚假信息非常有教育意义。例如，联邦调查局发现，俄罗斯的虚假信息活动经常组织两个分别代表问题两面的假“草根”抗议活动，并让它们同时举行！《休斯顿纪事报》报道了其中一个奇怪的事件（<<texas>>）。\n",
    "\n",
    "> ：一个自称为“德克萨斯之心”的团体在社交媒体上组织了这次抗议活动，他们说，这是反对“德克萨斯伊斯兰化”的抗议。在特拉维斯街的一边，我发现了大约10名抗议者。在另一边，我发现了大约50名反抗议者。但我找不到集会的组织者。没有“德克萨斯之心”。我认为这很奇怪，并在文章中提到了这一点：什么样的团体会在自己的活动中缺席？现在我知道为什么了。显然，集会的组织者当时在俄罗斯圣彼得堡。“德克萨斯之心”是特别检察官罗伯特·穆勒最近起诉试图干预美国总统选举的俄罗斯人中提到的互联网巨魔团体之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ethics/image13.png\" id=\"texas\" caption=\"Event organized by the group Heart of Texas\" alt=\"Screenshot of an event organized by the group Heart of Texas\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 虚假信息通常涉及协调一致的不真实行为活动。例如，欺诈性账户可能试图让人觉得许多人持有某种特定观点。虽然我们大多数人喜欢认为自己是独立思考的，但实际上，我们进化成受我们群体内的人影响，反对我们群体外的人。在线讨论可以影响我们的观点，或者改变我们认为可接受的观点范围。人类是社会性动物，作为社会性动物，我们极易受到周围人的影响。越来越多的激进化现象发生在在线环境中；影响来自在线论坛和社交网络虚拟空间中的人。\n",
    "\n",
    "由于深度学习提供的极大增强能力，通过自动生成的文本进行虚假信息传播成为一个特别严重的问题。当我们深入探讨创建语言模型时，我们将详细讨论这个问题，参见<<chapter_nlp>>。\n",
    "\n",
    "一种提议的方法是开发某种形式的数字签名，以无缝的方式实施它，并创建我们应该只信任经过验证的内容的规范。艾伦人工智能研究所所长Oren Etzioni在一篇名为[\"How Will We Prevent AI-Based Forgery?\"](https://hbr.org/2019/03/how-will-we-prevent-ai-based-forgery)的文章中提出了这样的建议：“人工智能即将使高保真度的伪造变得廉价且自动化，这可能对民主、安全和社会造成潜在的灾难性后果。人工智能伪造的幽灵意味着我们需要采取行动，使数字签名成为数字内容认证的必需品。”\n",
    "\n",
    "虽然我们不能希望讨论深度学习以及更广泛的算法所带来的所有伦理问题，但希望这个简短的介绍能成为你可以在此基础上进一步了解的有用起点。现在我们将转向如何识别伦理问题以及如何处理这些问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 识别和解决道德问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 错误是难免的。发现错误并处理它们需要成为包括机器学习在内的任何系统设计的一部分（以及许多其他系统）。数据伦理中提出的问题通常是复杂且跨学科的，但我们必须努力解决这些问题。\n",
    "\n",
    "那么我们能做些什么呢？这是一个很大的话题，但解决伦理问题的几步措施包括：\n",
    "\n",
    "- 分析你正在工作的项目。\n",
    "- 在你的公司实施流程，以发现并解决伦理风险。\n",
    "- 支持良好的政策。\n",
    "- 增加多样性。\n",
    "\n",
    "让我们逐一了解这些步骤，从分析你正在工作的项目开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析你正在进行的项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在考虑工作的伦理影响时，很容易忽略重要的问题。帮助巨大的一件事就是简单地提出正确的问题。Rachel Thomas建议在数据项目的开发过程中考虑以下问题：\n",
    "\n",
    "- 我们真的应该做这个吗？\n",
    "- 数据中存在什么偏见？\n",
    "- 代码和数据可以被审计吗？\n",
    "- 不同子群体的错误率是多少？\n",
    "- 一个简单的基于规则的替代方案的准确性如何？\n",
    "- 有哪些流程来处理上诉或错误？\n",
    "- 构建它的团队有多多样化？\n",
    "\n",
    "这些问题可能有助于你识别未解决的问题，以及更容易理解和控制的可能替代方案。除了提出正确的问题，考虑实施的做法和流程也很重要。\n",
    "\n",
    "在这个阶段要考虑的一件事是你正在收集和存储哪些数据。数据通常最终会被用于与最初收集目的不同的目的。例如，IBM在大屠杀之前就开始向纳粹德国销售，包括帮助阿道夫·希特勒进行的1933年德国人口普查，这次普查有效地识别出比之前在德国已知的更多的犹太人。同样，美国人口普查数据在二战期间被用来围捕日裔美国人（他们是美国公民）进行拘禁。认识到收集的数据和图像可能在以后被武器化是非常重要的。哥伦比亚大学教授[Tim Wu写道](https://www.nytimes.com/2019/04/10/opinion/sunday/privacy-capitalism.html)：“你必须假设Facebook或Android保留的任何个人数据都是世界各国政府会试图获取的数据，或者小偷会试图窃取的数据。”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实施流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 马克库拉中心发布了[工程/设计实践的伦理工具包](https://www.scu.edu/ethics-in-technology-practice/ethical-toolkit/)，其中包含了一些可以在你的公司实施的具体做法，包括定期进行主动搜索伦理风险的扫描（类似于网络安全渗透测试），扩大伦理圈以包括各种利益相关者的观点，以及考虑那些糟糕的人（坏人如何滥用、盗窃、误解、黑客攻击、破坏或武器化你正在构建的东西？）。\n",
    "\n",
    "即使你没有一个多元化的团队，你仍然可以尝试主动包括更广泛群体的观点，考虑以下问题（由马克库拉中心提供）：\n",
    "\n",
    "- 我们是否仅仅假设了某些人的利益、愿望、技能、经验和价值观，而不是实际咨询？\n",
    "- 所有将直接受到我们产品影响的利益相关者是谁？他们的利益如何得到保护？我们怎么知道他们真正的利益是什么——我们问过吗？\n",
    "- 哪些群体和个人将以显著的方式间接受到影响？\n",
    "- 有哪些人可能会使用这个产品，我们没有预料到他们会使用，或者出于我们最初没有打算的目的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 伦理镜头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 马克库拉中心的另一个有用资源是其[技术和工程实践中的概念框架](https://www.scu.edu/ethics-in-technology-practice/ethical-lenses/)。这个框架考虑了不同的基础伦理视角如何帮助识别具体问题，并列出了以下方法和关键问题：\n",
    "\n",
    "- 权利方法：哪个选项最能尊重所有利益相关者的权利？\n",
    "- 正义方法：哪个选项能平等或按比例对待人们？\n",
    "- 功利主义方法：哪个选项能产生最大的好处并造成最小的伤害？\n",
    "- 公共利益方法：哪个选项最能服务于整个社区，而不仅仅是某些成员？\n",
    "- 德性方法：哪个选项能引导我成为我想成为的那种人？\n",
    "\n",
    "马克库拉的建议包括深入研究这些视角，包括通过其*后果*的镜头来看待项目：\n",
    "\n",
    "- 谁将直接受到这个项目的影响？谁将间接受到影响？\n",
    "- 总体效果是否可能产生的好处大于伤害，以及哪些类型的好和坏？\n",
    "- 我们是否考虑了所有相关类型的好/坏（心理、政治、环境、道德、认知、情感、制度、文化）？\n",
    "- 未来的世代可能如何受到这个项目的影响？\n",
    "- 这个项目的风险是否不成比例地落在社会上最无力的人身上？好处是否会不成比例地流向富裕者？\n",
    "- 我们是否充分考虑了“双重用途”？\n",
    "\n",
    "与此相对的是*义务论*视角，它关注基本的*对*和*错*概念：\n",
    "\n",
    "- 我们必须尊重他人的哪些权利和义务？\n",
    "- 这个项目可能如何影响每个利益相关者的尊严和自主权？\n",
    "- 信任和正义的相关考虑因素是什么？\n",
    "- 这个项目是否涉及对他人的冲突道德义务或冲突的利益相关者权利？我们如何优先考虑这些？\n",
    "\n",
    "帮助提出完整和深思熟虑的答案的最好方法之一是确保提出问题的人*多样化*。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元化的力量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前，据Element AI的一项研究显示，少于12%的人工智能研究人员是女性。当涉及种族和年龄时，统计数据同样令人堪忧。当一个团队的所有成员背景相似时，他们很可能对伦理风险具有相似的盲点。《哈佛商业评论》（HBR）发表了许多关于多元化团队的研究，包括：\n",
    "\n",
    "- [\"多样性如何推动创新\"](https://hbr.org/2013/12/how-diversity-can-drive-innovation)\n",
    "- [\"团队在认知多样性更高时解决问题更快\"](https://hbr.org/2017/03/teams-solve-problems-faster-when-theyre-more-cognitively-diverse)\n",
    "- [\"为什么多元化的团队更聪明\"](https://hbr.org/2016/11/why-diverse-teams-are-smarter)，以及\n",
    "- [\"捍卫你的研究：什么让一个团队更聪明？更多女性\"](https://hbr.org/2011/06/defend-your-research-what-makes-a-team-smarter-more-women)\n",
    "\n",
    "多样性可以导致问题更早被识别，并考虑到更广泛的解决方案。例如，Tracy Chou是Quora的一名早期工程师。她在描述自己的经历时写道，她在公司内部为添加一个允许阻止恶意用户的功能进行了倡导。Chou回忆道：“我渴望着为该功能工作，因为我在网站上个人感觉到了敌对和虐待（性别是一个不太可能的原因）……但如果我没有那种个人视角，那么Quora团队可能不会在其存在的早期就优先考虑建立一个屏蔽按钮。”骚扰经常会驱使来自边缘化群体的人离开在线平台，因此这个功能对于维护Quora社区的健康非常重要。\n",
    "\n",
    "一个关键的要点是，女性离开科技行业的速度是男性的两倍以上，根据《哈佛商业评论》（Harvard Business Review）的数据（41%的科技行业女性离开，而男性则为17%）。对200多本书籍、白皮书和文章的分析发现，她们离开的原因是“她们受到不公平对待；工资较低，不像她们的男性同事那样可能被加快晋升，无法晋升。”\n",
    "\n",
    "研究证实了一些使女性在职场上更难晋升的因素。女性在绩效评估中更多地受到模糊的反馈和个性批评，而男性则得到与业务结果相关的可操作建议（更有用）。女性经常经历被排除在更具创意和创新力的角色之外，并且没有获得有助于晋升的高能见度的“挑战”任务。一项研究发现，即使在阅读相同的脚本时，男性的声音被认为比女性的声音更具有说服力、基于事实和逻辑。\n",
    "\n",
    "接受导师指导在统计学上被证明有助于男性晋升，但不利于女性。背后的原因是，当女性接受导师指导时，她们得到的是如何改变和获得更多自我认识的建议。当男性接受导师指导时，那是对他们权威的公开认可。猜猜哪个更有助于晋升？\n",
    "\n",
    "只要合格的女性继续退出科\n",
    "\n",
    "技行业，教更多女孩编程将无法解决困扰该领域的多样性问题。尽管女性面临许多额外的障碍，但多样性倡议往往主要关注白人女性。在对60名从事STEM研究的女性进行的[采访](https://worklifelaw.org/publications/Double-Jeopardy-Report_v6_full_web-sm.pdf)中，所有人都曾经历过歧视。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "招聘流程在科技行业中特别破碎。一项表明这种功能障碍的研究来自Triplebyte，这是一家帮助将软件工程师安置在公司中的公司，作为这个过程的一部分进行了标准化的技术面试。他们有一个非常有趣的数据集：超过300名工程师在考试中的表现结果，以及这些工程师在各种公司的面试过程中的表现结果。来自[Triplebyte研究](https://triplebyte.com/blog/who-y-combinator-companies-want)的最重要的发现是，“每家公司寻找的程序员类型通常与公司实际需要或业务无关。相反，它们反映了公司文化和创始人的背景。”\n",
    "\n",
    "对于试图进入深度学习世界的人来说，这是一个挑战，因为大多数公司的深度学习团队今天都是由学者创立的。这些团队倾向于寻找“像他们一样”的人——也就是说，能够解决复杂的数学问题并理解密集的行话的人。他们并不总是知道如何辨认出那些真正擅长使用深度学习解决实际问题的人。\n",
    "\n",
    "这为那些愿意超越地位和血统、专注于结果的公司提供了一个巨大的机会！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 公平、问责和透明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算机科学家的专业学会ACM（Association for Computing Machinery）组织了一个名为“公平、责任和透明度大会”的数据伦理会议。曾经被称为*FAT*的“公平、责任和透明度”现在使用更不引人反感的缩写*FAccT*。微软有一个专注于“公平、责任、透明度和道德”的团队（FATE）。在本节中，我们将使用*FAccT*来指代*公平、责任和透明度*的概念。\n",
    "\n",
    "*FAccT*是另一个你可能会发现在考虑伦理问题时有用的视角。一个有用的资源是Solon Barocas、Moritz Hardt和Arvind Narayanan的免费在线书籍[*《公平与机器学习：局限与机会》*](https://fairmlbook.org/)，该书“以将公平视为一个核心问题而不是事后想法的方式来看待机器学习”。然而，它也警告说，“这本书在范围上是有意狭窄的……对于技术人员和企业来说，狭隘的机器学习伦理框架可能是一种诱人的方式，可以专注于技术干预，而规避有关权力和责任的更深层次的问题。我们警告不要陷入这种诱惑。”与其提供*FAccT*方法对伦理问题的概述（这样的概述最好在类似的书籍中完成），我们在这里的重点将放在这种狭隘框架的局限性上。\n",
    "\n",
    "评估一个伦理视角是否完整的一个好方法是尝试想出一个例子，其中该视角和我们自己的伦理直觉得出不同的结果。Os Keyes、Jevan Hutson和Meredith Durbin在他们的论文[*“一项碎片提案：分析和改进一个将老年人转化为高营养泥浆的算法系统”*](https://arxiv.org/abs/1908.06166)中以图形方式探讨了这一点。论文的摘要如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">：算法系统的伦理影响在人机交互领域和对技术设计、开发和政策感兴趣的更广泛社区中都得到了广泛讨论。在本文中，我们探讨了一个著名的伦理框架——公平、责任和透明度——对一个提出的算法的应用，该算法解决了围绕食品安全和人口老龄化的各种社会问题。通过使用各种标准化的算法审计和评估形式，我们大大增加了算法对*FAT*框架的遵循程度，从而实现了一个更加伦理和有益的系统。我们讨论了这如何可以成为其他研究人员或从业者的指南，他们希望确保在他们的工作中的算法系统产生更好的伦理结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这篇论文中，相当具有争议的提案（“将老年人转化为高营养泥浆”）和结果（“大大增加算法对*FAT*框架的遵循程度，从而实现了一个更具伦理和有益的系统”）存在明显的矛盾...至少可以这么说！\n",
    "\n",
    "在哲学中，特别是在伦理学中，这是最有效的工具之一：首先，提出一个流程、定义、一组问题等，旨在解决某些问题。然后尝试想出一个例子，其中这种表面上的解决方案会导致一个没有人会认为可接受的提案。这样可以进一步完善解决方案。\n",
    "\n",
    "到目前为止，我们已经专注于你和你的组织可以采取的行动。但有时，个人或组织的行动是不够的。有时，政府也需要考虑政策的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 政策的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们经常与那些渴望技术或设计修复成为解决我们讨论过的问题的完整解决方案的人交谈；例如，对去偏见数据的技术方法，或者设计指南来使技术变得不那么容易上瘾。尽管这些措施可能有用，但它们不足以解决导致我们当前状态的根本问题。例如，只要创造上瘾的技术非常有利可图，公司就会继续这样做，而不管这是否会导致促进阴谋论并污染我们的信息生态系统。尽管个别设计师可能试图调整产品设计，但在基本的利润激励机制发生变化之前，我们不会看到实质性的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监管的有效性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要看看是什么促使公司采取具体行动，可以考虑以下两个关于Facebook行为的例子。2018年，联合国的一项调查发现Facebook在缅甸罗兴亚人种族灭绝中发挥了“决定性作用”，这是联合国秘书长安东尼奥·古特雷斯所描述的“世界上最受歧视的人之一”。自2013年以来，当地的活动人士一直在警告Facebook高管，他们的平台被用来传播仇恨言论和煽动暴力。2015年，他们被警告说Facebook可能在缅甸发挥与1994年卢旺达种族灭绝期间广播电台相同的作用（那里有一百万人被杀害）。然而，到2015年底，Facebook只雇用了四名会说缅甸语的合同工。一位与此事关系密切的人说：“这不是事后诸葛亮。这个问题的规模是巨大的，而且已经显而易见。”扎克伯格在国会听证会上承诺雇佣“数十名”人员来应对缅甸的种族灭绝（2018年，灭绝已经开始多年，包括2017年8月后至少摧毁了北部拉敏邦州的288个村庄）。\n",
    "\n",
    "这与Facebook迅速在德国[雇佣了1200人](http://thehill.com/policy/technology/361722-facebook-opens-second-german-office-to-comply-with-hate-speech-law)试图避免在一项针对仇恨言论的新德国法律下面临高达5000万欧元的昂贵罚款的情况形成鲜明对比。显然，在这种情况下，Facebook更加关注对财务罚款的威胁，而不是对一个种族的系统性毁灭。\n",
    "\n",
    "在一篇关于隐私问题的[文章](https://idlewords.com/2019/06/the_new_wilderness.htm)中，马西耶克·塞格洛夫斯基与环保运动进行了类比：\n",
    "\n",
    ">：这个监管项目在第一世界已经取得了如此成功的成果，以至于我们冒着忘记它之前生活是什么样的风险。像今天在雅加达和德里杀死数千人的浓烟一样，曾经是伦敦的象征。俄亥俄州的库亚霍加河曾经经常着火。在一个特别可怕的无法预料到的后果的例子中，汽油中添加的四乙基铅在全球范围内提高了五十年的暴力犯罪率。这些伤害都无法通过告诉人们用钱投票、仔细审查他们交易的每个公司的环保政策、或者停止使用相关技术来解决。这需要跨越辖区界限的协调和有时是高度技术化的监管来解决。在一些情况下，比如[臭氧层消耗的商业制冷剂禁令](https://en.wikipedia.org/wiki/Montreal_Protocol)，这种监管需要全球共识。我们已经到了需要在隐私法方面进行类似的视角转变的时候。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权利和政策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清洁的空气和清洁的饮用水是几乎不可能通过个人市场决策来保护的公共产品，而是需要协调的监管行动。同样，许多技术误用所导致的意外后果涉及公共产品，如污染的信息环境或恶化的环境隐私。隐私往往被框定为个人权利，然而广泛监控带来的社会影响是存在的（即使个别个体有可能选择退出）。\n",
    "\n",
    "我们在技术领域看到的许多问题实际上是人权问题，比如一个有偏见的算法建议黑人被告应该获得更长的监禁期，特定的工作广告只向年轻人显示，或者警方使用面部识别来识别抗议者。解决人权问题的适当途径通常是通过法律。\n",
    "\n",
    "我们需要监管和法律的变革，以及个人的道德行为。个人行为的改变无法解决利润动机不一致、外部性（即企业获取巨额利润，同时将成本和危害转嫁给更广泛的社会）或系统性失败。然而，法律永远不可能涵盖所有边际案例，因此重要的是个人软件开发人员和数据科学家具备在实践中做出道德决策的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 汽车：历史先例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们面临的问题是复杂的，没有简单的解决方案。这可能令人沮丧，但我们在考虑历史上人们应对过的其他重大挑战时找到了希望。一个例子是提高汽车安全性的运动，这在Timnit Gebru等人的论文《数据集的数据表》和设计播客99% Invisible中都有涉及。早期的汽车没有安全带，仪表盘上的金属旋钮可能会在撞车时刺入人的头骨，普通的平板玻璃窗以危险的方式破碎，而不可折叠的方向盘则会刺穿驾驶员的身体。然而，汽车公司非常抵制甚至讨论安全作为他们可以帮助解决的问题的想法，普遍的观点是汽车就是它们的样子，导致问题的是使用它们的人。\n",
    "\n",
    "消费者安全活动家和倡导者花了几十年的时间才改变了国家对于或许汽车公司有责任应通过监管解决这一问题的讨论。当可折叠式方向盘被发明时，由于没有财务激励，它并未立即实施。主要汽车公司通用汽车雇佣了私家侦探试图挖掘消费者安全倡导者拉尔夫·纳德的丑闻。安全带、碰撞测试假人和可折叠式方向盘的要求是重大的胜利。直到2011年，汽车公司才被要求开始使用能够代表普通女性，而不仅仅是普通男性身体的碰撞测试假人；在此之前，女性在相同冲击的汽车事故中受伤的可能性比男性高40%。这是偏见、政策和技术产生重要影响的生动例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从处理二进制逻辑的背景出发，伦理学中缺乏明确的答案可能起初会令人沮丧。然而，我们的工作如何影响世界，包括意外后果以及工作被恶意行为者利用的影响，是我们可以（也应该！）考虑的最重要的问题之一。尽管没有简单的答案，但有明确的陷阱要避免和实践要遵循，以朝着更加道德的行为迈进。\n",
    "\n",
    "许多人（包括我们自己！）都在寻找关于如何解决技术的有害影响的更加令人满意、坚实的答案。然而，考虑到我们面临的问题的复杂性、深远性和跨学科性质，没有简单的解决方案。Julia Angwin，前ProPublica高级记者，专注于算法偏见和监视问题（也是2016年调查COMPAS累犯算法的调查员之一，这一调查帮助引发了FAccT领域），在[2019年的一次采访](https://www.fastcompany.com/90337954/who-cares-about-liberty-julia-angwin-and-trevor-paglen-on-privacy-surveillance-and-the-mess-were-in)中表示：\n",
    "\n",
    "> 我坚信，要解决一个问题，你必须对其进行诊断，而我们仍处于这一阶段。如果你想想世纪之交和工业化，我们有30年的童工、无限制的工作时间、可怕的工作条件，需要很多记者揭发丑闻和倡导，来诊断问题并对其有一些了解，然后进行活动以改变法律。我觉得我们正在经历数据信息的第二次工业化...... 我认为我的角色是尽可能清晰地表明这些负面影响，并对其进行准确的诊断，以便能够解决。这是艰苦的工作，需要更多的人来做。\n",
    "\n",
    "令人欣慰的是，Angwin认为我们大部分时间仍处于诊断阶段：如果你对这些问题的理解感觉不完整，这是正常和自然的。目前还没有“疗法”，尽管我们继续努力更好地理解和解决我们面临的问题至关重要。\n",
    "\n",
    "我们这本书的一位审阅人，Fred Monroe，曾在对冲基金交易方面工作。他在阅读了本章后告诉我们，这里讨论的许多问题（数据分布与模型训练所用的数据截然不同，模型一旦部署并大规模使用后的影响反馈循环等等）也是构建盈利交易模型的关键问题。考虑到社会后果需要做的事情与考虑组织、市场和客户后果的事情有很多重叠之处，因此认真思考伦理问题也可以帮助你认真思考如何使你的数据产品在更广泛的范围内取得成功！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问卷调查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 伦理学是否提供了一份“正确答案”的清单？\n",
    "2. 与不同背景的人合作在考虑伦理问题时如何有所帮助？\n",
    "3. IBM在纳粹德国扮演了什么角色？为什么公司会以此方式参与？工人为什么参与？\n",
    "4. 柴油丑闻中第一个被监禁的人的角色是什么？\n",
    "5. 加州执法部门维护的涉嫌帮派成员数据库存在什么问题？\n",
    "6. 为什么YouTube的推荐算法会向恋童癖者推荐部分裸体儿童的视频，尽管谷歌的员工没有编写此功能？\n",
    "7. 指标的中心化存在哪些问题？\n",
    "8. 为什么Meetup.com在其技术聚会推荐系统中不包括性别？\n",
    "9. 根据Suresh和Guttag，机器学习中有哪六种偏见？\n",
    "10. 给出两个美国历史上种族偏见的例子。\n",
    "11. ImageNet中的大多数图像来自哪里？\n",
    "12. 在[《机器学习是否自动化道德风险和错误》](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf)一文中，为什么鼻窦炎被发现与中风有关？\n",
    "13. 什么是代表性偏见？\n",
    "14. 机器和人在决策使用上有何不同？\n",
    "15. 虚假信息是否与“假新闻”相同？\n",
    "16. 为什么通过自动生成的文本进行虚假信息传播是一个特别重要的问题？\n",
    "17. Markkula中心描述的五种伦理视角是什么？\n",
    "18. 在哪些方面政策是解决数据伦理问题的合适工具？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进一步研究："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 阅读文章《当算法介入你的医疗保健时会发生什么》。未来如何避免类似问题？\n",
    "1. 研究更多关于 YouTube 推荐系统及其社会影响的信息。你认为推荐系统是否总是会带来负面结果的反馈循环？Google 可以采取哪些方法来避免这种情况？政府呢？\n",
    "1. 阅读论文[\"Discrimination in Online Ad Delivery\"](https://arxiv.org/abs/1301.6822)。你认为 Google 应对 Sweeney 博士所遭受的事件负责吗？适当的回应是什么？\n",
    "1. 跨学科团队如何帮助避免负面后果？\n",
    "1. 阅读论文\"Does Machine Learning Automate Moral Hazard and Error\"。你认为应采取何种行动来处理该论文中指出的问题？\n",
    "1. 阅读文章\"How Will We Prevent AI-Based Forgery?\"。你认为 Etzioni 提出的方法可行吗？为什么？\n",
    "1. 在本章节中完成“分析你正在进行的项目”部分。\n",
    "1. 考虑你的团队是否可以更加多元化。如果可以，有哪些方法可能会有所帮助？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习实践：总结！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜！你已经完成了本书的第一部分。在这一部分中，我们试图向你展示深度学习可以做什么，以及你如何使用它来创建真正的应用和产品。现在，如果你花一些时间尝试所学的知识，你将会从本书中获得更多的收获。也许你在学习的过程中已经在尝试这些知识了——如果是这样，那太棒了！如果没有，也没有关系……现在是开始尝试的好时机。\n",
    "\n",
    "如果你还没有访问过[书籍网站](https://book.fast.ai)，现在就去那里看看吧。确保你已经设置好运行笔记本的环境。成为一名有效的深度学习实践者的关键在于实践，所以你需要训练模型。因此，如果你还没有运行笔记本，请立即去运行！此外，请在网站上查看任何重要的更新或通知；深度学习发展迅速，我们无法更改书中打印的文字，因此网站是你查看最新信息的地方。\n",
    "\n",
    "确保你已经完成以下步骤：\n",
    "\n",
    "- 连接到书籍网站推荐的一个 GPU Jupyter 服务器。\n",
    "- 自己运行第一个笔记本。\n",
    "- 上传在第一个笔记本中找到的图像；然后尝试一些不同类型的图像，看看会发生什么。\n",
    "- 运行第二个笔记本，根据你提出的图像搜索查询收集自己的数据集。\n",
    "- 思考如何使用深度学习来帮助你自己的项目，包括你可以使用的数据类型，可能遇到的问题以及如何在实践中解决这些问题。\n",
    "\n",
    "在本书的下一部分中，你将了解深度学习的工作原理和原因，而不仅仅是看到你如何在实践中使用它。了解深度学习的工作原理和原因对于从业者和研究人员都很重要，因为在这个相对较新的领域，几乎每个项目都需要一定程度的定制和调试。你对深度学习的基础理解越深入，你的模型就越好。这些基础对于高管、产品经理等人来说不那么重要（尽管仍然有用，所以请继续阅读！），但对于那些实际上正在训练和部署模型的人来说至关重要。"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
